"""Reddit tools for listing posts in a subreddit."""

import urllib.parse
import urllib.request
import urllib.error
import json
from datetime import datetime, timezone

MAX_ITEMS = 25
TIMEOUT = 30


def _fetch_json(url, headers=None):
    """Fetch URL and parse as JSON."""
    req_headers = {"User-Agent": "research-tool/1.0"}
    if headers:
        req_headers.update(headers)
    req = urllib.request.Request(url, headers=req_headers)
    try:
        with urllib.request.urlopen(req, timeout=TIMEOUT) as resp:
            return json.loads(resp.read().decode('utf-8'))
    except urllib.error.HTTPError as e:
        return {"error": "HTTP %d: %s" % (e.code, e.reason)}
    except urllib.error.URLError as e:
        return {"error": "URL error: %s" % str(e.reason)}
    except Exception as e:
        return {"error": str(e)}


def _to_iso8601(created_utc):
    if created_utc is None:
        return ""
    try:
        dt = datetime.fromtimestamp(float(created_utc), tz=timezone.utc)
        return dt.isoformat().replace("+00:00", "Z")
    except Exception:
        return ""


def reddit_list(subreddit: str,
                sort: str = "hot",
                t: str = "day",
                limit: int = MAX_ITEMS):
    """List posts in a subreddit by sort and (optionally) time window.

    Uses Reddit's listing endpoints:
    - /r/<subreddit>/hot.json
    - /r/<subreddit>/new.json
    - /r/<subreddit>/top.json?t=day|week|month|year|all
    - /r/<subreddit>/rising.json

    Args:
      subreddit: Subreddit name, with or without "r/" prefix.
      sort: One of "hot", "new", "top", "rising".
      t: Time window for "top" sort: "hour", "day", "week", "month",
         "year", "all". Ignored for other sorts.
      limit: Number of posts to return (max 100).

    Returns:
      Dict containing subreddit, sort, t, and a list of posts with title,
      url, permalink, score, comments, author, created, and truncated
      selftext.
    """
    subreddit = (subreddit or "").strip()
    if subreddit.lower().startswith("r/"):
        subreddit = subreddit[2:]
    subreddit = subreddit.strip("/")

    if not subreddit:
        return {"error": "subreddit is required"}

    sort = (sort or "hot").strip().lower()
    allowed_sorts = {"hot", "new", "top", "rising"}
    if sort not in allowed_sorts:
        return {"error": "Invalid sort: %s" % sort}

    t = (t or "day").strip().lower()
    allowed_t = {"hour", "day", "week", "month", "year", "all"}
    if t not in allowed_t:
        return {"error": "Invalid time window t: %s" % t}

    limit = int(limit) if limit is not None else MAX_ITEMS
    limit = max(1, min(limit, 100))

    base_url = "https://www.reddit.com/r/%s/%s.json" % (
        urllib.parse.quote(subreddit),
        urllib.parse.quote(sort),
    )
    qs = {"limit": str(limit)}
    if sort == "top":
        qs["t"] = t
    url = base_url + "?" + urllib.parse.urlencode(qs)

    data = _fetch_json(url)
    if "error" in data:
        return data

    children = data.get("data", {}).get("children", [])
    posts = []
    for child in children[:limit]:
        post = (child or {}).get("data", {}) or {}
        title = post.get("title", "") or ""
        permalink = post.get("permalink", "") or ""
        selftext = post.get("selftext", "") or ""
        if len(selftext) > 500:
            selftext = selftext[:500] + "â€¦"

        posts.append({
            "title": title,
            "subreddit": post.get("subreddit", subreddit) or subreddit,
            "author": post.get("author", "") or "",
            "score": post.get("score", 0),
            "comments": post.get("num_comments", 0),
            "created": _to_iso8601(post.get("created_utc")),
            "url": post.get("url", "") or "",
            "permalink": "https://reddit.com%s" % permalink if permalink else "",
            "selftext": selftext,
        })

    return {
        "subreddit": subreddit,
        "sort": sort,
        "t": t if sort == "top" else "",
        "limit": limit,
        "posts": posts,
        "url": url,
    }


reddit_list.safe = True
